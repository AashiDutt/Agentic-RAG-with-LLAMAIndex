{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Building an Agent Reasoning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "\n",
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "To download this paper, below is the needed code:\n",
    "\n",
    "#!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O metagpt.pdf\n",
    "\n",
    "**Note**: The pdf file is included with this lesson. To access it, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Query Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code for utils.py\n",
    "\n",
    "# # TODO: abstract all of this into a function that takes in a PDF file name \n",
    "\n",
    "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex\n",
    "# from llama_index.core.node_parser import SentenceSplitter\n",
    "# from llama_index.core.tools import FunctionTool, QueryEngineTool\n",
    "# from llama_index.core.vector_stores import MetadataFilters, FilterCondition\n",
    "# from typing import List, Optional\n",
    "\n",
    "# def get_doc_tools(\n",
    "#     file_path: str,\n",
    "#     name: str,\n",
    "# ) -> str:\n",
    "#     \"\"\"Get vector query and summary query tools from a document.\"\"\"\n",
    "\n",
    "#     # load documents\n",
    "#     documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "#     splitter = SentenceSplitter(chunk_size=1024)\n",
    "#     nodes = splitter.get_nodes_from_documents(documents)\n",
    "#     vector_index = VectorStoreIndex(nodes)\n",
    "    \n",
    "#     def vector_query(\n",
    "#         query: str, \n",
    "#         page_numbers: Optional[List[str]] = None\n",
    "#     ) -> str:\n",
    "#         \"\"\"Use to answer questions over the MetaGPT paper.\n",
    "    \n",
    "#         Useful if you have specific questions over the MetaGPT paper.\n",
    "#         Always leave page_numbers as None UNLESS there is a specific page you want to search for.\n",
    "    \n",
    "#         Args:\n",
    "#             query (str): the string query to be embedded.\n",
    "#             page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE \n",
    "#                 if we want to perform a vector search\n",
    "#                 over all pages. Otherwise, filter by the set of specified pages.\n",
    "        \n",
    "#         \"\"\"\n",
    "    \n",
    "#         page_numbers = page_numbers or []\n",
    "#         metadata_dicts = [\n",
    "#             {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "#         ]\n",
    "        \n",
    "#         query_engine = vector_index.as_query_engine(\n",
    "#             similarity_top_k=2,\n",
    "#             filters=MetadataFilters.from_dicts(\n",
    "#                 metadata_dicts,\n",
    "#                 condition=FilterCondition.OR\n",
    "#             )\n",
    "#         )\n",
    "#         response = query_engine.query(query)\n",
    "#         return response\n",
    "        \n",
    "    \n",
    "#     vector_query_tool = FunctionTool.from_defaults(\n",
    "#         name=f\"vector_tool_{name}\",\n",
    "#         fn=vector_query\n",
    "#     )\n",
    "    \n",
    "#     summary_index = SummaryIndex(nodes)\n",
    "#     summary_query_engine = summary_index.as_query_engine(\n",
    "#         response_mode=\"tree_summarize\",\n",
    "#         use_async=True,\n",
    "#     )\n",
    "#     summary_tool = QueryEngineTool.from_defaults(\n",
    "#         name=f\"summary_tool_{name}\",\n",
    "#         query_engine=summary_query_engine,\n",
    "#         description=(\n",
    "#             \"Use ONLY IF you want to get a holistic summary of MetaGPT. \"\n",
    "#             \"Do NOT use if you have specific questions over MetaGPT.\"\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "#     return vector_query_tool, summary_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_doc_tools\n",
    "\n",
    "vector_tool, summary_tool = get_doc_tools(\"metagpt.pdf\", \"metagpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup Function Calling Agent\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool, summary_tool], \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)\n",
    "\n",
    "response = agent.query(\n",
    "    \"Tell me about the agent roles in MetaGPT, \"\n",
    "    \"and then how they communicate with each other.\"\n",
    ")\n",
    "\n",
    "print(response.source_nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.chat(\n",
    "    \"Tell me about the evaluation datasets used.\"\n",
    ")\n",
    "\n",
    "response = agent.chat(\"Tell me the results over one of the above datasets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lower-Level: Debuggability and Control\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool, summary_tool], \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = agent.create_task(\n",
    "    \"Tell me about the agent roles in MetaGPT, \"\n",
    "    \"and then how they communicate with each other.\"\n",
    ")\n",
    "\n",
    "step_output = agent.run_step(task.task_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_steps = agent.get_completed_steps(task.task_id)\n",
    "print(f\"Num completed for task {task.task_id}: {len(completed_steps)}\")\n",
    "print(completed_steps[0].output.sources[0].raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upcoming_steps = agent.get_upcoming_steps(task.task_id)\n",
    "print(f\"Num upcoming steps for task {task.task_id}: {len(upcoming_steps)}\")\n",
    "upcoming_steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_output = agent.run_step(\n",
    "    task.task_id, input=\"What about how agents share information?\"\n",
    ")\n",
    "\n",
    "step_output = agent.run_step(task.task_id)\n",
    "print(step_output.is_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.finalize_response(task.task_id)\n",
    "\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
